{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "import imagenet_classes\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "model.eval()  # set in eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image\n",
    "\n",
    "size = (224, 224)\n",
    "\n",
    "single_image_name = 'peppers.jpg'\n",
    "img_as_img = Image.open(single_image_name).convert('RGB')\n",
    "resize = transforms.Resize(size=size)  # define resize fn\n",
    "img_as_img = resize(img_as_img)  # resize image\n",
    "\n",
    "toTensor = transforms.ToTensor()  # define tensor transform\n",
    "img_as_tensor = toTensor(img_as_img)  # conver to tensor\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "img_as_tensor = normalize(img_as_tensor)\n",
    "img_as_tensor = torch.unsqueeze(img_as_tensor, 0)  # add 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. bell pepper, probability: 0.9867660999298096\n",
      "2. cucumber, cuke, probability: 0.008986140601336956\n",
      "3. grocery store, grocery, food market, market, probability: 0.001201258390210569\n"
     ]
    }
   ],
   "source": [
    "# forward prop\n",
    "logits = model(img_as_tensor)  \n",
    "probs_out = torch.nn.functional.softmax(logits, dim=1)  # run softmax to get probs\n",
    "probs, preds = torch.topk(probs_out.data, 3)  # grab top 3\n",
    "\n",
    "# print(probs.shape)\n",
    "\n",
    "for ind, pred in enumerate(preds.numpy()[0]):\n",
    "    print('{}. {}, probability: {}'.format(ind+1, imagenet_classes.classes[pred], probs[0][ind]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.children())\n",
    "\n",
    "def forward(x):\n",
    "\n",
    "    select = [1,18,36]  # select these layer numbers from network\n",
    "#     select = [4]\n",
    "\n",
    "    features = []  # a list of feature maps\n",
    "    \n",
    "    for ind, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if ind in select:\n",
    "            features.append(x)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \n",
    "    min_val = torch.min(x)\n",
    "    range_val = torch.max(x) - min_val\n",
    "    \n",
    "    if range_val > 0:\n",
    "        # broad cast subtraction and division\n",
    "        normalized = (x - min_val) / range_val\n",
    "    else:\n",
    "        normalized = torch.zeros(x.size())\n",
    "        \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# features is a list of 4d tensor (layer, batch, channel, heigh, width)\n",
    "layers = forward(img_as_tensor)\n",
    "\n",
    "# processed feat maps\n",
    "layer_feature_maps = []\n",
    "\n",
    "\n",
    "# loop through list of layers\n",
    "for layer_num, layer in enumerate(layers):\n",
    "    \n",
    "    feature_maps = layer.squeeze()  # only batch size of 1\n",
    "    processed_feature_maps = []\n",
    "    \n",
    "    # loop through feat maps (in that layer)\n",
    "    for feature_map in feature_maps:\n",
    "        \n",
    "        feature_map = normalize(feature_map.detach())\n",
    "        \n",
    "        # detach and normalize\n",
    "        processed_feature_maps.append(feature_map)\n",
    "\n",
    "    fig, axarr = plt.subplots(nrows=8, ncols=8, figsize=(15,15))\n",
    "\n",
    "    feat_map_idx = 0\n",
    "\n",
    "    # iterate through rows and cols, and put 2d feat map inside\n",
    "    for row in axarr:\n",
    "        for col in row:\n",
    "\n",
    "            col.imshow(processed_feature_maps[feat_map_idx])\n",
    "            feat_map_idx += 1\n",
    "\n",
    "    plt.savefig('feature_map_{}'.format(layer_num))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Written answer part1b\n",
    "\n",
    "The structure of the feature map is early on is relatively high resolution of the image.\n",
    "\n",
    "Beginning layer:\n",
    "The 5 features I chose appear to activate certain colors in the original image, or at least\n",
    "the over all shape of the image.  One feature map activates mostly red colors. Another\n",
    "activates green colors.  One map activates all the peppers, so just the foreground.\n",
    "\n",
    "Middle layer:\n",
    "This feature map becomes already hard to interpret, but appears to map where\n",
    "certain objects are located in the image in general.  But I can't tell\n",
    "what the maps are activating beyond that, perhaps curves.\n",
    "\n",
    "Later layer:\n",
    "These feature maps are uninterpretable.  Perhaps they represent what type\n",
    "of object lies in that area, or parts of an object.\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
