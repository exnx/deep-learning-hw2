{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # sees 32, 32, 3\n",
    "        self.conv1 = nn.Conv2d(3, 64, 11, padding=5)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # sees 16, 16, 64\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # sees 16, 16, 128\n",
    "        self.conv3 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.batch_norm_mean_pool = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # will get flattened to 128\n",
    "        self.fc1 = nn.Linear(128, 10)\n",
    "        \n",
    "        \n",
    "    \n",
    "        # sees 16, 16, 128 -> will flatten to 128\n",
    "        self.global_mean_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.max_pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "        \n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        \n",
    "        x = F.relu(self.batch_norm3(self.conv3(x)))\n",
    "\n",
    "        # global mean pool\n",
    "        x = self.global_mean_pool(x)\n",
    "        \n",
    "        # flatten (remove the 1 dimensions)\n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        x = self.batch_norm_mean_pool(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CIFAR image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network object\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer and loss function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "  (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batch_norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_mean_pool): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (global_mean_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move to gpu\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [1/1000], Loss: 2.3015\n",
      "Epoch [1/150], Step [101/1000], Loss: 2.2406\n",
      "Epoch [1/150], Step [201/1000], Loss: 2.1414\n",
      "Epoch [1/150], Step [301/1000], Loss: 2.1390\n",
      "Epoch [1/150], Step [401/1000], Loss: 2.1205\n",
      "Epoch [1/150], Step [501/1000], Loss: 2.1148\n",
      "Epoch [1/150], Step [601/1000], Loss: 1.9734\n",
      "Epoch [1/150], Step [701/1000], Loss: 2.0385\n",
      "Epoch [1/150], Step [801/1000], Loss: 2.0093\n",
      "Epoch [1/150], Step [901/1000], Loss: 2.0009\n",
      "Epoch [1/150], Train Loss: 2.1009\n",
      "Epoch [1/150], Test Loss: 2.0442\n",
      "Test Accuracy: 41.59 %\n",
      "Epoch [2/150], Step [1/1000], Loss: 2.0271\n",
      "Epoch [2/150], Step [101/1000], Loss: 2.0539\n",
      "Epoch [2/150], Step [201/1000], Loss: 1.9827\n",
      "Epoch [2/150], Step [301/1000], Loss: 2.0336\n",
      "Epoch [2/150], Step [401/1000], Loss: 1.8810\n",
      "Epoch [2/150], Step [501/1000], Loss: 1.9870\n",
      "Epoch [2/150], Step [601/1000], Loss: 1.8895\n",
      "Epoch [2/150], Step [701/1000], Loss: 1.9674\n",
      "Epoch [2/150], Step [801/1000], Loss: 2.0263\n",
      "Epoch [2/150], Step [901/1000], Loss: 1.9079\n",
      "Epoch [2/150], Train Loss: 1.9808\n",
      "Epoch [2/150], Test Loss: 1.9936\n",
      "Test Accuracy: 47.2 %\n",
      "Epoch [3/150], Step [1/1000], Loss: 2.1190\n",
      "Epoch [3/150], Step [101/1000], Loss: 1.9635\n",
      "Epoch [3/150], Step [201/1000], Loss: 1.9889\n",
      "Epoch [3/150], Step [301/1000], Loss: 1.9721\n",
      "Epoch [3/150], Step [401/1000], Loss: 1.9200\n",
      "Epoch [3/150], Step [501/1000], Loss: 1.8848\n",
      "Epoch [3/150], Step [601/1000], Loss: 1.9331\n",
      "Epoch [3/150], Step [701/1000], Loss: 1.8778\n",
      "Epoch [3/150], Step [801/1000], Loss: 1.9570\n",
      "Epoch [3/150], Step [901/1000], Loss: 1.9491\n",
      "Epoch [3/150], Train Loss: 1.9282\n",
      "Epoch [3/150], Test Loss: 1.9234\n",
      "Test Accuracy: 54.03 %\n",
      "Epoch [4/150], Step [1/1000], Loss: 1.8740\n",
      "Epoch [4/150], Step [101/1000], Loss: 1.8637\n",
      "Epoch [4/150], Step [201/1000], Loss: 1.9435\n",
      "Epoch [4/150], Step [301/1000], Loss: 1.9058\n",
      "Epoch [4/150], Step [401/1000], Loss: 1.9706\n",
      "Epoch [4/150], Step [501/1000], Loss: 1.8781\n",
      "Epoch [4/150], Step [601/1000], Loss: 1.8000\n",
      "Epoch [4/150], Step [701/1000], Loss: 1.9588\n",
      "Epoch [4/150], Step [801/1000], Loss: 1.8182\n",
      "Epoch [4/150], Step [901/1000], Loss: 1.8986\n",
      "Epoch [4/150], Train Loss: 1.8914\n",
      "Epoch [4/150], Test Loss: 1.9078\n",
      "Test Accuracy: 55.96 %\n",
      "Epoch [5/150], Step [1/1000], Loss: 1.9948\n",
      "Epoch [5/150], Step [101/1000], Loss: 1.8348\n",
      "Epoch [5/150], Step [201/1000], Loss: 2.0308\n",
      "Epoch [5/150], Step [301/1000], Loss: 1.9197\n",
      "Epoch [5/150], Step [401/1000], Loss: 1.8683\n",
      "Epoch [5/150], Step [501/1000], Loss: 1.9122\n",
      "Epoch [5/150], Step [601/1000], Loss: 1.7773\n",
      "Epoch [5/150], Step [701/1000], Loss: 1.9924\n",
      "Epoch [5/150], Step [801/1000], Loss: 1.8615\n",
      "Epoch [5/150], Step [901/1000], Loss: 1.7647\n",
      "Epoch [5/150], Train Loss: 1.8677\n",
      "Epoch [5/150], Test Loss: 1.8865\n",
      "Test Accuracy: 57.86 %\n",
      "Epoch [6/150], Step [1/1000], Loss: 1.7382\n",
      "Epoch [6/150], Step [101/1000], Loss: 1.8673\n",
      "Epoch [6/150], Step [201/1000], Loss: 1.8184\n",
      "Epoch [6/150], Step [301/1000], Loss: 1.8881\n",
      "Epoch [6/150], Step [401/1000], Loss: 1.8205\n",
      "Epoch [6/150], Step [501/1000], Loss: 1.7508\n",
      "Epoch [6/150], Step [601/1000], Loss: 1.8637\n",
      "Epoch [6/150], Step [701/1000], Loss: 1.8526\n",
      "Epoch [6/150], Step [801/1000], Loss: 1.8913\n",
      "Epoch [6/150], Step [901/1000], Loss: 1.7255\n",
      "Epoch [6/150], Train Loss: 1.8455\n",
      "Epoch [6/150], Test Loss: 1.8584\n",
      "Test Accuracy: 60.33 %\n",
      "Epoch [7/150], Step [1/1000], Loss: 1.8219\n",
      "Epoch [7/150], Step [101/1000], Loss: 1.8186\n",
      "Epoch [7/150], Step [201/1000], Loss: 1.7680\n",
      "Epoch [7/150], Step [301/1000], Loss: 1.7635\n",
      "Epoch [7/150], Step [401/1000], Loss: 1.7941\n",
      "Epoch [7/150], Step [501/1000], Loss: 1.9085\n",
      "Epoch [7/150], Step [601/1000], Loss: 1.8467\n",
      "Epoch [7/150], Step [701/1000], Loss: 1.8779\n",
      "Epoch [7/150], Step [801/1000], Loss: 1.8586\n",
      "Epoch [7/150], Step [901/1000], Loss: 1.8329\n",
      "Epoch [7/150], Train Loss: 1.8283\n",
      "Epoch [7/150], Test Loss: 1.8401\n",
      "Test Accuracy: 62.82 %\n",
      "Epoch [8/150], Step [1/1000], Loss: 1.8583\n",
      "Epoch [8/150], Step [101/1000], Loss: 1.8051\n",
      "Epoch [8/150], Step [201/1000], Loss: 1.8319\n",
      "Epoch [8/150], Step [301/1000], Loss: 1.9067\n",
      "Epoch [8/150], Step [401/1000], Loss: 1.8937\n",
      "Epoch [8/150], Step [501/1000], Loss: 1.8963\n",
      "Epoch [8/150], Step [601/1000], Loss: 1.7762\n",
      "Epoch [8/150], Step [701/1000], Loss: 1.7295\n",
      "Epoch [8/150], Step [801/1000], Loss: 1.8856\n",
      "Epoch [8/150], Step [901/1000], Loss: 1.7727\n",
      "Epoch [8/150], Train Loss: 1.8118\n",
      "Epoch [8/150], Test Loss: 1.8369\n",
      "Test Accuracy: 62.44 %\n",
      "Epoch [9/150], Step [1/1000], Loss: 1.8082\n",
      "Epoch [9/150], Step [101/1000], Loss: 1.7936\n",
      "Epoch [9/150], Step [201/1000], Loss: 1.8314\n",
      "Epoch [9/150], Step [301/1000], Loss: 1.8961\n",
      "Epoch [9/150], Step [401/1000], Loss: 1.7653\n",
      "Epoch [9/150], Step [501/1000], Loss: 1.8129\n",
      "Epoch [9/150], Step [601/1000], Loss: 1.8420\n",
      "Epoch [9/150], Step [701/1000], Loss: 1.8630\n",
      "Epoch [9/150], Step [801/1000], Loss: 1.7687\n",
      "Epoch [9/150], Step [901/1000], Loss: 1.7719\n",
      "Epoch [9/150], Train Loss: 1.7996\n",
      "Epoch [9/150], Test Loss: 1.8045\n",
      "Test Accuracy: 65.89 %\n",
      "Epoch [10/150], Step [1/1000], Loss: 1.6810\n",
      "Epoch [10/150], Step [101/1000], Loss: 1.8786\n",
      "Epoch [10/150], Step [201/1000], Loss: 1.8795\n",
      "Epoch [10/150], Step [301/1000], Loss: 1.7031\n",
      "Epoch [10/150], Step [401/1000], Loss: 1.8726\n",
      "Epoch [10/150], Step [501/1000], Loss: 1.7734\n",
      "Epoch [10/150], Step [601/1000], Loss: 1.7275\n",
      "Epoch [10/150], Step [701/1000], Loss: 1.8065\n",
      "Epoch [10/150], Step [801/1000], Loss: 1.7605\n",
      "Epoch [10/150], Step [901/1000], Loss: 1.7967\n",
      "Epoch [10/150], Train Loss: 1.7823\n",
      "Epoch [10/150], Test Loss: 1.7917\n",
      "Test Accuracy: 67.34 %\n",
      "Epoch [11/150], Step [1/1000], Loss: 1.7281\n",
      "Epoch [11/150], Step [101/1000], Loss: 1.8653\n",
      "Epoch [11/150], Step [201/1000], Loss: 1.7180\n",
      "Epoch [11/150], Step [301/1000], Loss: 1.7240\n",
      "Epoch [11/150], Step [401/1000], Loss: 1.7788\n",
      "Epoch [11/150], Step [501/1000], Loss: 1.7624\n",
      "Epoch [11/150], Step [601/1000], Loss: 1.6720\n",
      "Epoch [11/150], Step [701/1000], Loss: 1.8679\n",
      "Epoch [11/150], Step [801/1000], Loss: 1.7706\n",
      "Epoch [11/150], Step [901/1000], Loss: 1.8036\n",
      "Epoch [11/150], Train Loss: 1.7723\n",
      "Epoch [11/150], Test Loss: 1.8022\n",
      "Test Accuracy: 66.11 %\n",
      "Epoch [12/150], Step [1/1000], Loss: 1.7622\n",
      "Epoch [12/150], Step [101/1000], Loss: 1.7033\n",
      "Epoch [12/150], Step [201/1000], Loss: 1.7586\n",
      "Epoch [12/150], Step [301/1000], Loss: 1.8878\n",
      "Epoch [12/150], Step [401/1000], Loss: 1.7613\n",
      "Epoch [12/150], Step [501/1000], Loss: 1.7140\n",
      "Epoch [12/150], Step [601/1000], Loss: 1.7832\n",
      "Epoch [12/150], Step [701/1000], Loss: 1.6818\n",
      "Epoch [12/150], Step [801/1000], Loss: 1.7846\n",
      "Epoch [12/150], Step [901/1000], Loss: 1.6623\n",
      "Epoch [12/150], Train Loss: 1.7595\n",
      "Epoch [12/150], Test Loss: 1.8000\n",
      "Test Accuracy: 66.19 %\n",
      "Epoch [13/150], Step [1/1000], Loss: 1.7059\n",
      "Epoch [13/150], Step [101/1000], Loss: 1.6314\n",
      "Epoch [13/150], Step [201/1000], Loss: 1.7273\n",
      "Epoch [13/150], Step [301/1000], Loss: 1.8267\n",
      "Epoch [13/150], Step [401/1000], Loss: 1.6451\n",
      "Epoch [13/150], Step [501/1000], Loss: 1.7436\n",
      "Epoch [13/150], Step [601/1000], Loss: 1.7844\n",
      "Epoch [13/150], Step [701/1000], Loss: 1.7416\n",
      "Epoch [13/150], Step [801/1000], Loss: 1.8098\n",
      "Epoch [13/150], Step [901/1000], Loss: 1.6966\n",
      "Epoch [13/150], Train Loss: 1.7462\n",
      "Epoch [13/150], Test Loss: 1.7770\n",
      "Test Accuracy: 68.69 %\n",
      "Epoch [14/150], Step [1/1000], Loss: 1.6419\n",
      "Epoch [14/150], Step [101/1000], Loss: 1.7290\n",
      "Epoch [14/150], Step [201/1000], Loss: 1.7930\n",
      "Epoch [14/150], Step [301/1000], Loss: 1.7858\n",
      "Epoch [14/150], Step [401/1000], Loss: 1.7005\n",
      "Epoch [14/150], Step [501/1000], Loss: 1.8919\n",
      "Epoch [14/150], Step [601/1000], Loss: 1.7856\n",
      "Epoch [14/150], Step [701/1000], Loss: 1.6381\n",
      "Epoch [14/150], Step [801/1000], Loss: 1.8195\n",
      "Epoch [14/150], Step [901/1000], Loss: 1.7902\n",
      "Epoch [14/150], Train Loss: 1.7372\n",
      "Epoch [14/150], Test Loss: 1.7691\n",
      "Test Accuracy: 69.37 %\n",
      "Epoch [15/150], Step [1/1000], Loss: 1.8113\n",
      "Epoch [15/150], Step [101/1000], Loss: 1.7098\n",
      "Epoch [15/150], Step [201/1000], Loss: 1.6629\n",
      "Epoch [15/150], Step [301/1000], Loss: 1.7479\n",
      "Epoch [15/150], Step [401/1000], Loss: 1.6117\n",
      "Epoch [15/150], Step [501/1000], Loss: 1.7035\n",
      "Epoch [15/150], Step [601/1000], Loss: 1.6950\n",
      "Epoch [15/150], Step [701/1000], Loss: 1.7872\n",
      "Epoch [15/150], Step [801/1000], Loss: 1.6743\n",
      "Epoch [15/150], Step [901/1000], Loss: 1.6865\n",
      "Epoch [15/150], Train Loss: 1.7261\n",
      "Epoch [15/150], Test Loss: 1.7532\n",
      "Test Accuracy: 71.1 %\n",
      "Epoch [16/150], Step [1/1000], Loss: 1.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/150], Step [101/1000], Loss: 1.7238\n",
      "Epoch [16/150], Step [201/1000], Loss: 1.7147\n",
      "Epoch [16/150], Step [301/1000], Loss: 1.6875\n",
      "Epoch [16/150], Step [401/1000], Loss: 1.7892\n",
      "Epoch [16/150], Step [501/1000], Loss: 1.7152\n",
      "Epoch [16/150], Step [601/1000], Loss: 1.7385\n",
      "Epoch [16/150], Step [701/1000], Loss: 1.7608\n",
      "Epoch [16/150], Step [801/1000], Loss: 1.6967\n",
      "Epoch [16/150], Step [901/1000], Loss: 1.7461\n",
      "Epoch [16/150], Train Loss: 1.7180\n",
      "Epoch [16/150], Test Loss: 1.7478\n",
      "Test Accuracy: 71.66 %\n",
      "Epoch [17/150], Step [1/1000], Loss: 1.7408\n",
      "Epoch [17/150], Step [101/1000], Loss: 1.8273\n",
      "Epoch [17/150], Step [201/1000], Loss: 1.6937\n",
      "Epoch [17/150], Step [301/1000], Loss: 1.6339\n",
      "Epoch [17/150], Step [401/1000], Loss: 1.7248\n",
      "Epoch [17/150], Step [501/1000], Loss: 1.6646\n",
      "Epoch [17/150], Step [601/1000], Loss: 1.6041\n",
      "Epoch [17/150], Step [701/1000], Loss: 1.6937\n",
      "Epoch [17/150], Step [801/1000], Loss: 1.7535\n",
      "Epoch [17/150], Step [901/1000], Loss: 1.7431\n",
      "Epoch [17/150], Train Loss: 1.7069\n",
      "Epoch [17/150], Test Loss: 1.7573\n",
      "Test Accuracy: 70.57 %\n",
      "Epoch [18/150], Step [1/1000], Loss: 1.6652\n",
      "Epoch [18/150], Step [101/1000], Loss: 1.6129\n",
      "Epoch [18/150], Step [201/1000], Loss: 1.7757\n",
      "Epoch [18/150], Step [301/1000], Loss: 1.7592\n",
      "Epoch [18/150], Step [401/1000], Loss: 1.7629\n",
      "Epoch [18/150], Step [501/1000], Loss: 1.8320\n",
      "Epoch [18/150], Step [601/1000], Loss: 1.6940\n",
      "Epoch [18/150], Step [701/1000], Loss: 1.7414\n",
      "Epoch [18/150], Step [801/1000], Loss: 1.6784\n",
      "Epoch [18/150], Step [901/1000], Loss: 1.6829\n",
      "Epoch [18/150], Train Loss: 1.7018\n",
      "Epoch [18/150], Test Loss: 1.7205\n",
      "Test Accuracy: 74.26 %\n",
      "Epoch [19/150], Step [1/1000], Loss: 1.7612\n",
      "Epoch [19/150], Step [101/1000], Loss: 1.6424\n",
      "Epoch [19/150], Step [201/1000], Loss: 1.6919\n",
      "Epoch [19/150], Step [301/1000], Loss: 1.7473\n",
      "Epoch [19/150], Step [401/1000], Loss: 1.6654\n",
      "Epoch [19/150], Step [501/1000], Loss: 1.5657\n",
      "Epoch [19/150], Step [601/1000], Loss: 1.6199\n",
      "Epoch [19/150], Step [701/1000], Loss: 1.6878\n",
      "Epoch [19/150], Step [801/1000], Loss: 1.7325\n",
      "Epoch [19/150], Step [901/1000], Loss: 1.6946\n",
      "Epoch [19/150], Train Loss: 1.6922\n",
      "Epoch [19/150], Test Loss: 1.7376\n",
      "Test Accuracy: 72.55 %\n",
      "Epoch [20/150], Step [1/1000], Loss: 1.7821\n",
      "Epoch [20/150], Step [101/1000], Loss: 1.7250\n",
      "Epoch [20/150], Step [201/1000], Loss: 1.6694\n",
      "Epoch [20/150], Step [301/1000], Loss: 1.7395\n",
      "Epoch [20/150], Step [401/1000], Loss: 1.7168\n",
      "Epoch [20/150], Step [501/1000], Loss: 1.7238\n",
      "Epoch [20/150], Step [601/1000], Loss: 1.6291\n",
      "Epoch [20/150], Step [701/1000], Loss: 1.7446\n",
      "Epoch [20/150], Step [801/1000], Loss: 1.6602\n",
      "Epoch [20/150], Step [901/1000], Loss: 1.6316\n",
      "Epoch [20/150], Train Loss: 1.6843\n",
      "Epoch [20/150], Test Loss: 1.7254\n",
      "Test Accuracy: 74.12 %\n",
      "Epoch [21/150], Step [1/1000], Loss: 1.6290\n",
      "Epoch [21/150], Step [101/1000], Loss: 1.7587\n",
      "Epoch [21/150], Step [201/1000], Loss: 1.6280\n",
      "Epoch [21/150], Step [301/1000], Loss: 1.6907\n",
      "Epoch [21/150], Step [401/1000], Loss: 1.6443\n",
      "Epoch [21/150], Step [501/1000], Loss: 1.6809\n",
      "Epoch [21/150], Step [601/1000], Loss: 1.6379\n",
      "Epoch [21/150], Step [701/1000], Loss: 1.6975\n",
      "Epoch [21/150], Step [801/1000], Loss: 1.6923\n",
      "Epoch [21/150], Step [901/1000], Loss: 1.6235\n",
      "Epoch [21/150], Train Loss: 1.6795\n",
      "Epoch [21/150], Test Loss: 1.7472\n",
      "Test Accuracy: 71.78 %\n",
      "Epoch [22/150], Step [1/1000], Loss: 1.6205\n",
      "Epoch [22/150], Step [101/1000], Loss: 1.6435\n",
      "Epoch [22/150], Step [201/1000], Loss: 1.6640\n",
      "Epoch [22/150], Step [301/1000], Loss: 1.6659\n",
      "Epoch [22/150], Step [401/1000], Loss: 1.6618\n",
      "Epoch [22/150], Step [501/1000], Loss: 1.6230\n",
      "Epoch [22/150], Step [601/1000], Loss: 1.7035\n",
      "Epoch [22/150], Step [701/1000], Loss: 1.6937\n",
      "Epoch [22/150], Step [801/1000], Loss: 1.8099\n",
      "Epoch [22/150], Step [901/1000], Loss: 1.7789\n",
      "Epoch [22/150], Train Loss: 1.6750\n",
      "Epoch [22/150], Test Loss: 1.7312\n",
      "Test Accuracy: 73.15 %\n",
      "Epoch [23/150], Step [1/1000], Loss: 1.6199\n",
      "Epoch [23/150], Step [101/1000], Loss: 1.7704\n",
      "Epoch [23/150], Step [201/1000], Loss: 1.7207\n",
      "Epoch [23/150], Step [301/1000], Loss: 1.7047\n",
      "Epoch [23/150], Step [401/1000], Loss: 1.6608\n",
      "Epoch [23/150], Step [501/1000], Loss: 1.6535\n",
      "Epoch [23/150], Step [601/1000], Loss: 1.7446\n",
      "Epoch [23/150], Step [701/1000], Loss: 1.6496\n",
      "Epoch [23/150], Step [801/1000], Loss: 1.6479\n",
      "Epoch [23/150], Step [901/1000], Loss: 1.7635\n",
      "Epoch [23/150], Train Loss: 1.6660\n",
      "Epoch [23/150], Test Loss: 1.7201\n",
      "Test Accuracy: 74.45 %\n",
      "Epoch [24/150], Step [1/1000], Loss: 1.7680\n",
      "Epoch [24/150], Step [101/1000], Loss: 1.6058\n",
      "Epoch [24/150], Step [201/1000], Loss: 1.7131\n",
      "Epoch [24/150], Step [301/1000], Loss: 1.6747\n",
      "Epoch [24/150], Step [401/1000], Loss: 1.6038\n",
      "Epoch [24/150], Step [501/1000], Loss: 1.6654\n",
      "Epoch [24/150], Step [601/1000], Loss: 1.5829\n",
      "Epoch [24/150], Step [701/1000], Loss: 1.6033\n",
      "Epoch [24/150], Step [801/1000], Loss: 1.6395\n",
      "Epoch [24/150], Step [901/1000], Loss: 1.7429\n",
      "Epoch [24/150], Train Loss: 1.6588\n",
      "Epoch [24/150], Test Loss: 1.7132\n",
      "Test Accuracy: 75.05 %\n",
      "Epoch [25/150], Step [1/1000], Loss: 1.6763\n",
      "Epoch [25/150], Step [101/1000], Loss: 1.6051\n",
      "Epoch [25/150], Step [201/1000], Loss: 1.7127\n",
      "Epoch [25/150], Step [301/1000], Loss: 1.7043\n",
      "Epoch [25/150], Step [401/1000], Loss: 1.6722\n",
      "Epoch [25/150], Step [501/1000], Loss: 1.7016\n",
      "Epoch [25/150], Step [601/1000], Loss: 1.6345\n",
      "Epoch [25/150], Step [701/1000], Loss: 1.6131\n",
      "Epoch [25/150], Step [801/1000], Loss: 1.6969\n",
      "Epoch [25/150], Step [901/1000], Loss: 1.5923\n",
      "Epoch [25/150], Train Loss: 1.6571\n",
      "Epoch [25/150], Test Loss: 1.7251\n",
      "Test Accuracy: 73.97 %\n",
      "Epoch [26/150], Step [1/1000], Loss: 1.6833\n",
      "Epoch [26/150], Step [101/1000], Loss: 1.6735\n",
      "Epoch [26/150], Step [201/1000], Loss: 1.6329\n",
      "Epoch [26/150], Step [301/1000], Loss: 1.6777\n",
      "Epoch [26/150], Step [401/1000], Loss: 1.6531\n",
      "Epoch [26/150], Step [501/1000], Loss: 1.6878\n",
      "Epoch [26/150], Step [601/1000], Loss: 1.6273\n",
      "Epoch [26/150], Step [701/1000], Loss: 1.6399\n",
      "Epoch [26/150], Step [801/1000], Loss: 1.6129\n",
      "Epoch [26/150], Step [901/1000], Loss: 1.5611\n",
      "Epoch [26/150], Train Loss: 1.6494\n",
      "Epoch [26/150], Test Loss: 1.7044\n",
      "Test Accuracy: 75.66 %\n",
      "Epoch [27/150], Step [1/1000], Loss: 1.6891\n",
      "Epoch [27/150], Step [101/1000], Loss: 1.6042\n",
      "Epoch [27/150], Step [201/1000], Loss: 1.6099\n",
      "Epoch [27/150], Step [301/1000], Loss: 1.6251\n",
      "Epoch [27/150], Step [401/1000], Loss: 1.6621\n",
      "Epoch [27/150], Step [501/1000], Loss: 1.7181\n",
      "Epoch [27/150], Step [601/1000], Loss: 1.6847\n",
      "Epoch [27/150], Step [701/1000], Loss: 1.5856\n",
      "Epoch [27/150], Step [801/1000], Loss: 1.5245\n",
      "Epoch [27/150], Step [901/1000], Loss: 1.7089\n",
      "Epoch [27/150], Train Loss: 1.6455\n",
      "Epoch [27/150], Test Loss: 1.7086\n",
      "Test Accuracy: 75.44 %\n",
      "Epoch [28/150], Step [1/1000], Loss: 1.6210\n",
      "Epoch [28/150], Step [101/1000], Loss: 1.5413\n",
      "Epoch [28/150], Step [201/1000], Loss: 1.6967\n",
      "Epoch [28/150], Step [301/1000], Loss: 1.6034\n",
      "Epoch [28/150], Step [401/1000], Loss: 1.6232\n",
      "Epoch [28/150], Step [501/1000], Loss: 1.6205\n",
      "Epoch [28/150], Step [601/1000], Loss: 1.6253\n",
      "Epoch [28/150], Step [701/1000], Loss: 1.6692\n",
      "Epoch [28/150], Step [801/1000], Loss: 1.6705\n",
      "Epoch [28/150], Step [901/1000], Loss: 1.6578\n",
      "Epoch [28/150], Train Loss: 1.6404\n",
      "Epoch [28/150], Test Loss: 1.7051\n",
      "Test Accuracy: 75.79 %\n",
      "Epoch [29/150], Step [1/1000], Loss: 1.6794\n",
      "Epoch [29/150], Step [101/1000], Loss: 1.7111\n",
      "Epoch [29/150], Step [201/1000], Loss: 1.6111\n",
      "Epoch [29/150], Step [301/1000], Loss: 1.6210\n",
      "Epoch [29/150], Step [401/1000], Loss: 1.6538\n",
      "Epoch [29/150], Step [501/1000], Loss: 1.6378\n",
      "Epoch [29/150], Step [601/1000], Loss: 1.6511\n",
      "Epoch [29/150], Step [701/1000], Loss: 1.6216\n",
      "Epoch [29/150], Step [801/1000], Loss: 1.6646\n",
      "Epoch [29/150], Step [901/1000], Loss: 1.7150\n",
      "Epoch [29/150], Train Loss: 1.6354\n",
      "Epoch [29/150], Test Loss: 1.7073\n",
      "Test Accuracy: 75.37 %\n",
      "Epoch [30/150], Step [1/1000], Loss: 1.6408\n",
      "Epoch [30/150], Step [101/1000], Loss: 1.6081\n",
      "Epoch [30/150], Step [201/1000], Loss: 1.5402\n",
      "Epoch [30/150], Step [301/1000], Loss: 1.5922\n",
      "Epoch [30/150], Step [401/1000], Loss: 1.7092\n",
      "Epoch [30/150], Step [501/1000], Loss: 1.6102\n",
      "Epoch [30/150], Step [601/1000], Loss: 1.5457\n",
      "Epoch [30/150], Step [701/1000], Loss: 1.6538\n",
      "Epoch [30/150], Step [801/1000], Loss: 1.6727\n",
      "Epoch [30/150], Step [901/1000], Loss: 1.6033\n",
      "Epoch [30/150], Train Loss: 1.6318\n",
      "Epoch [30/150], Test Loss: 1.7012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.28 %\n",
      "Epoch [31/150], Step [1/1000], Loss: 1.6493\n",
      "Epoch [31/150], Step [101/1000], Loss: 1.6104\n",
      "Epoch [31/150], Step [201/1000], Loss: 1.6215\n",
      "Epoch [31/150], Step [301/1000], Loss: 1.6768\n",
      "Epoch [31/150], Step [401/1000], Loss: 1.5262\n",
      "Epoch [31/150], Step [501/1000], Loss: 1.6055\n",
      "Epoch [31/150], Step [601/1000], Loss: 1.6434\n",
      "Epoch [31/150], Step [701/1000], Loss: 1.6556\n",
      "Epoch [31/150], Step [801/1000], Loss: 1.5999\n",
      "Epoch [31/150], Step [901/1000], Loss: 1.6202\n",
      "Epoch [31/150], Train Loss: 1.6257\n",
      "Epoch [31/150], Test Loss: 1.6973\n",
      "Test Accuracy: 76.41 %\n",
      "Epoch [32/150], Step [1/1000], Loss: 1.5708\n",
      "Epoch [32/150], Step [101/1000], Loss: 1.5688\n",
      "Epoch [32/150], Step [201/1000], Loss: 1.6535\n",
      "Epoch [32/150], Step [301/1000], Loss: 1.5634\n",
      "Epoch [32/150], Step [401/1000], Loss: 1.5378\n",
      "Epoch [32/150], Step [501/1000], Loss: 1.6138\n",
      "Epoch [32/150], Step [601/1000], Loss: 1.6068\n",
      "Epoch [32/150], Step [701/1000], Loss: 1.7288\n",
      "Epoch [32/150], Step [801/1000], Loss: 1.6527\n",
      "Epoch [32/150], Step [901/1000], Loss: 1.6826\n",
      "Epoch [32/150], Train Loss: 1.6236\n",
      "Epoch [32/150], Test Loss: 1.6946\n",
      "Test Accuracy: 76.75 %\n",
      "Epoch [33/150], Step [1/1000], Loss: 1.6080\n",
      "Epoch [33/150], Step [101/1000], Loss: 1.6190\n",
      "Epoch [33/150], Step [201/1000], Loss: 1.6245\n",
      "Epoch [33/150], Step [301/1000], Loss: 1.5406\n",
      "Epoch [33/150], Step [401/1000], Loss: 1.5640\n",
      "Epoch [33/150], Step [501/1000], Loss: 1.6137\n",
      "Epoch [33/150], Step [601/1000], Loss: 1.6347\n",
      "Epoch [33/150], Step [701/1000], Loss: 1.6431\n",
      "Epoch [33/150], Step [801/1000], Loss: 1.6480\n",
      "Epoch [33/150], Step [901/1000], Loss: 1.5828\n",
      "Epoch [33/150], Train Loss: 1.6181\n",
      "Epoch [33/150], Test Loss: 1.6995\n",
      "Test Accuracy: 76.23 %\n",
      "Epoch [34/150], Step [1/1000], Loss: 1.5914\n",
      "Epoch [34/150], Step [101/1000], Loss: 1.6109\n",
      "Epoch [34/150], Step [201/1000], Loss: 1.6144\n",
      "Epoch [34/150], Step [301/1000], Loss: 1.5608\n",
      "Epoch [34/150], Step [401/1000], Loss: 1.5973\n",
      "Epoch [34/150], Step [501/1000], Loss: 1.5685\n",
      "Epoch [34/150], Step [601/1000], Loss: 1.6123\n",
      "Epoch [34/150], Step [701/1000], Loss: 1.6738\n",
      "Epoch [34/150], Step [801/1000], Loss: 1.6074\n",
      "Epoch [34/150], Step [901/1000], Loss: 1.6245\n",
      "Epoch [34/150], Train Loss: 1.6154\n",
      "Epoch [34/150], Test Loss: 1.6993\n",
      "Test Accuracy: 76.34 %\n",
      "Epoch [35/150], Step [1/1000], Loss: 1.5371\n",
      "Epoch [35/150], Step [101/1000], Loss: 1.6136\n",
      "Epoch [35/150], Step [201/1000], Loss: 1.6677\n",
      "Epoch [35/150], Step [301/1000], Loss: 1.6137\n",
      "Epoch [35/150], Step [401/1000], Loss: 1.6584\n",
      "Epoch [35/150], Step [501/1000], Loss: 1.5595\n",
      "Epoch [35/150], Step [601/1000], Loss: 1.6391\n",
      "Epoch [35/150], Step [701/1000], Loss: 1.6495\n",
      "Epoch [35/150], Step [801/1000], Loss: 1.7442\n",
      "Epoch [35/150], Step [901/1000], Loss: 1.5909\n",
      "Epoch [35/150], Train Loss: 1.6127\n",
      "Epoch [35/150], Test Loss: 1.6875\n",
      "Test Accuracy: 77.43 %\n",
      "Epoch [36/150], Step [1/1000], Loss: 1.5857\n",
      "Epoch [36/150], Step [101/1000], Loss: 1.6518\n",
      "Epoch [36/150], Step [201/1000], Loss: 1.5584\n",
      "Epoch [36/150], Step [301/1000], Loss: 1.6669\n",
      "Epoch [36/150], Step [401/1000], Loss: 1.6367\n",
      "Epoch [36/150], Step [501/1000], Loss: 1.6042\n",
      "Epoch [36/150], Step [601/1000], Loss: 1.5564\n",
      "Epoch [36/150], Step [701/1000], Loss: 1.6258\n",
      "Epoch [36/150], Step [801/1000], Loss: 1.5967\n",
      "Epoch [36/150], Step [901/1000], Loss: 1.6428\n",
      "Epoch [36/150], Train Loss: 1.6078\n",
      "Epoch [36/150], Test Loss: 1.6881\n",
      "Test Accuracy: 77.39 %\n",
      "Epoch [37/150], Step [1/1000], Loss: 1.5801\n",
      "Epoch [37/150], Step [101/1000], Loss: 1.6390\n",
      "Epoch [37/150], Step [201/1000], Loss: 1.5468\n",
      "Epoch [37/150], Step [301/1000], Loss: 1.6040\n",
      "Epoch [37/150], Step [401/1000], Loss: 1.5550\n",
      "Epoch [37/150], Step [501/1000], Loss: 1.6108\n",
      "Epoch [37/150], Step [601/1000], Loss: 1.6043\n",
      "Epoch [37/150], Step [701/1000], Loss: 1.6949\n",
      "Epoch [37/150], Step [801/1000], Loss: 1.6432\n",
      "Epoch [37/150], Step [901/1000], Loss: 1.6376\n",
      "Epoch [37/150], Train Loss: 1.6063\n",
      "Epoch [37/150], Test Loss: 1.6980\n",
      "Test Accuracy: 76.51 %\n",
      "Epoch [38/150], Step [1/1000], Loss: 1.6973\n",
      "Epoch [38/150], Step [101/1000], Loss: 1.5648\n",
      "Epoch [38/150], Step [201/1000], Loss: 1.7068\n",
      "Epoch [38/150], Step [301/1000], Loss: 1.5611\n",
      "Epoch [38/150], Step [401/1000], Loss: 1.5906\n",
      "Epoch [38/150], Step [501/1000], Loss: 1.6066\n",
      "Epoch [38/150], Step [601/1000], Loss: 1.5854\n",
      "Epoch [38/150], Step [701/1000], Loss: 1.5802\n",
      "Epoch [38/150], Step [801/1000], Loss: 1.5927\n",
      "Epoch [38/150], Step [901/1000], Loss: 1.5733\n",
      "Epoch [38/150], Train Loss: 1.6032\n",
      "Epoch [38/150], Test Loss: 1.6883\n",
      "Test Accuracy: 77.49 %\n",
      "Epoch [39/150], Step [1/1000], Loss: 1.5491\n",
      "Epoch [39/150], Step [101/1000], Loss: 1.5997\n",
      "Epoch [39/150], Step [201/1000], Loss: 1.5458\n",
      "Epoch [39/150], Step [301/1000], Loss: 1.6179\n",
      "Epoch [39/150], Step [401/1000], Loss: 1.6590\n",
      "Epoch [39/150], Step [501/1000], Loss: 1.5893\n",
      "Epoch [39/150], Step [601/1000], Loss: 1.6100\n",
      "Epoch [39/150], Step [701/1000], Loss: 1.5567\n",
      "Epoch [39/150], Step [801/1000], Loss: 1.5815\n",
      "Epoch [39/150], Step [901/1000], Loss: 1.5316\n",
      "Epoch [39/150], Train Loss: 1.5981\n",
      "Epoch [39/150], Test Loss: 1.6854\n",
      "Test Accuracy: 77.71 %\n",
      "Epoch [40/150], Step [1/1000], Loss: 1.6812\n",
      "Epoch [40/150], Step [101/1000], Loss: 1.5195\n",
      "Epoch [40/150], Step [201/1000], Loss: 1.6183\n",
      "Epoch [40/150], Step [301/1000], Loss: 1.6016\n",
      "Epoch [40/150], Step [401/1000], Loss: 1.5207\n",
      "Epoch [40/150], Step [501/1000], Loss: 1.6859\n",
      "Epoch [40/150], Step [601/1000], Loss: 1.6414\n",
      "Epoch [40/150], Step [701/1000], Loss: 1.6458\n",
      "Epoch [40/150], Step [801/1000], Loss: 1.5264\n",
      "Epoch [40/150], Step [901/1000], Loss: 1.6583\n",
      "Epoch [40/150], Train Loss: 1.5951\n",
      "Epoch [40/150], Test Loss: 1.6935\n",
      "Test Accuracy: 76.88 %\n",
      "Epoch [41/150], Step [1/1000], Loss: 1.5951\n",
      "Epoch [41/150], Step [101/1000], Loss: 1.5981\n",
      "Epoch [41/150], Step [201/1000], Loss: 1.5946\n",
      "Epoch [41/150], Step [301/1000], Loss: 1.5481\n",
      "Epoch [41/150], Step [401/1000], Loss: 1.6675\n",
      "Epoch [41/150], Step [501/1000], Loss: 1.5879\n",
      "Epoch [41/150], Step [601/1000], Loss: 1.5382\n",
      "Epoch [41/150], Step [701/1000], Loss: 1.7003\n",
      "Epoch [41/150], Step [801/1000], Loss: 1.5389\n",
      "Epoch [41/150], Step [901/1000], Loss: 1.5600\n",
      "Epoch [41/150], Train Loss: 1.5923\n",
      "Epoch [41/150], Test Loss: 1.6860\n",
      "Test Accuracy: 77.52 %\n",
      "Epoch [42/150], Step [1/1000], Loss: 1.5857\n",
      "Epoch [42/150], Step [101/1000], Loss: 1.5862\n",
      "Epoch [42/150], Step [201/1000], Loss: 1.5065\n",
      "Epoch [42/150], Step [301/1000], Loss: 1.6216\n",
      "Epoch [42/150], Step [401/1000], Loss: 1.5690\n",
      "Epoch [42/150], Step [501/1000], Loss: 1.7009\n",
      "Epoch [42/150], Step [601/1000], Loss: 1.6348\n",
      "Epoch [42/150], Step [701/1000], Loss: 1.5523\n",
      "Epoch [42/150], Step [801/1000], Loss: 1.5714\n",
      "Epoch [42/150], Step [901/1000], Loss: 1.6848\n",
      "Epoch [42/150], Train Loss: 1.5886\n",
      "Epoch [42/150], Test Loss: 1.6834\n",
      "Test Accuracy: 77.84 %\n",
      "Epoch [43/150], Step [1/1000], Loss: 1.5534\n",
      "Epoch [43/150], Step [101/1000], Loss: 1.5868\n",
      "Epoch [43/150], Step [201/1000], Loss: 1.5667\n",
      "Epoch [43/150], Step [301/1000], Loss: 1.6009\n",
      "Epoch [43/150], Step [401/1000], Loss: 1.6653\n",
      "Epoch [43/150], Step [501/1000], Loss: 1.6096\n",
      "Epoch [43/150], Step [601/1000], Loss: 1.6086\n",
      "Epoch [43/150], Step [701/1000], Loss: 1.5215\n",
      "Epoch [43/150], Step [801/1000], Loss: 1.5542\n",
      "Epoch [43/150], Step [901/1000], Loss: 1.5566\n",
      "Epoch [43/150], Train Loss: 1.5863\n",
      "Epoch [43/150], Test Loss: 1.6988\n",
      "Test Accuracy: 76.31 %\n",
      "Epoch [44/150], Step [1/1000], Loss: 1.5611\n",
      "Epoch [44/150], Step [101/1000], Loss: 1.5895\n",
      "Epoch [44/150], Step [201/1000], Loss: 1.5835\n",
      "Epoch [44/150], Step [301/1000], Loss: 1.5551\n",
      "Epoch [44/150], Step [401/1000], Loss: 1.5906\n",
      "Epoch [44/150], Step [501/1000], Loss: 1.6737\n",
      "Epoch [44/150], Step [601/1000], Loss: 1.5612\n",
      "Epoch [44/150], Step [701/1000], Loss: 1.5490\n",
      "Epoch [44/150], Step [801/1000], Loss: 1.5523\n",
      "Epoch [44/150], Step [901/1000], Loss: 1.5768\n",
      "Epoch [44/150], Train Loss: 1.5840\n",
      "Epoch [44/150], Test Loss: 1.6817\n",
      "Test Accuracy: 78.03 %\n",
      "Epoch [45/150], Step [1/1000], Loss: 1.5071\n",
      "Epoch [45/150], Step [101/1000], Loss: 1.6199\n",
      "Epoch [45/150], Step [201/1000], Loss: 1.5638\n",
      "Epoch [45/150], Step [301/1000], Loss: 1.5637\n",
      "Epoch [45/150], Step [401/1000], Loss: 1.6149\n",
      "Epoch [45/150], Step [501/1000], Loss: 1.6104\n",
      "Epoch [45/150], Step [601/1000], Loss: 1.6193\n",
      "Epoch [45/150], Step [701/1000], Loss: 1.5492\n",
      "Epoch [45/150], Step [801/1000], Loss: 1.6914\n",
      "Epoch [45/150], Step [901/1000], Loss: 1.5570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/150], Train Loss: 1.5825\n",
      "Epoch [45/150], Test Loss: 1.6827\n",
      "Test Accuracy: 78.17 %\n",
      "Epoch [46/150], Step [1/1000], Loss: 1.6140\n",
      "Epoch [46/150], Step [101/1000], Loss: 1.5955\n",
      "Epoch [46/150], Step [201/1000], Loss: 1.5614\n",
      "Epoch [46/150], Step [301/1000], Loss: 1.5296\n",
      "Epoch [46/150], Step [401/1000], Loss: 1.5231\n",
      "Epoch [46/150], Step [501/1000], Loss: 1.6142\n",
      "Epoch [46/150], Step [601/1000], Loss: 1.5748\n",
      "Epoch [46/150], Step [701/1000], Loss: 1.5049\n",
      "Epoch [46/150], Step [801/1000], Loss: 1.5592\n",
      "Epoch [46/150], Step [901/1000], Loss: 1.5803\n",
      "Epoch [46/150], Train Loss: 1.5799\n",
      "Epoch [46/150], Test Loss: 1.6916\n",
      "Test Accuracy: 77.14 %\n",
      "Epoch [47/150], Step [1/1000], Loss: 1.5645\n",
      "Epoch [47/150], Step [101/1000], Loss: 1.5789\n",
      "Epoch [47/150], Step [201/1000], Loss: 1.4922\n",
      "Epoch [47/150], Step [301/1000], Loss: 1.5733\n",
      "Epoch [47/150], Step [401/1000], Loss: 1.5901\n",
      "Epoch [47/150], Step [501/1000], Loss: 1.5382\n",
      "Epoch [47/150], Step [601/1000], Loss: 1.5913\n",
      "Epoch [47/150], Step [701/1000], Loss: 1.6075\n",
      "Epoch [47/150], Step [801/1000], Loss: 1.5927\n",
      "Epoch [47/150], Step [901/1000], Loss: 1.5818\n",
      "Epoch [47/150], Train Loss: 1.5765\n",
      "Epoch [47/150], Test Loss: 1.6853\n",
      "Test Accuracy: 77.63 %\n",
      "Epoch [48/150], Step [1/1000], Loss: 1.6021\n",
      "Epoch [48/150], Step [101/1000], Loss: 1.5729\n",
      "Epoch [48/150], Step [201/1000], Loss: 1.5837\n",
      "Epoch [48/150], Step [301/1000], Loss: 1.5115\n",
      "Epoch [48/150], Step [401/1000], Loss: 1.6209\n",
      "Epoch [48/150], Step [501/1000], Loss: 1.5693\n",
      "Epoch [48/150], Step [601/1000], Loss: 1.5846\n",
      "Epoch [48/150], Step [701/1000], Loss: 1.6312\n",
      "Epoch [48/150], Step [801/1000], Loss: 1.5637\n",
      "Epoch [48/150], Step [901/1000], Loss: 1.5962\n",
      "Epoch [48/150], Train Loss: 1.5751\n",
      "Epoch [48/150], Test Loss: 1.6909\n",
      "Test Accuracy: 76.93 %\n",
      "Epoch [49/150], Step [1/1000], Loss: 1.5823\n",
      "Epoch [49/150], Step [101/1000], Loss: 1.5685\n",
      "Epoch [49/150], Step [201/1000], Loss: 1.5310\n",
      "Epoch [49/150], Step [301/1000], Loss: 1.5967\n",
      "Epoch [49/150], Step [401/1000], Loss: 1.6049\n",
      "Epoch [49/150], Step [501/1000], Loss: 1.6139\n",
      "Epoch [49/150], Step [601/1000], Loss: 1.5783\n",
      "Epoch [49/150], Step [701/1000], Loss: 1.5867\n",
      "Epoch [49/150], Step [801/1000], Loss: 1.5585\n",
      "Epoch [49/150], Step [901/1000], Loss: 1.5241\n",
      "Epoch [49/150], Train Loss: 1.5702\n",
      "Epoch [49/150], Test Loss: 1.6817\n",
      "Test Accuracy: 78.02 %\n",
      "Epoch [50/150], Step [1/1000], Loss: 1.5767\n",
      "Epoch [50/150], Step [101/1000], Loss: 1.5925\n",
      "Epoch [50/150], Step [201/1000], Loss: 1.5799\n",
      "Epoch [50/150], Step [301/1000], Loss: 1.5999\n",
      "Epoch [50/150], Step [401/1000], Loss: 1.5603\n",
      "Epoch [50/150], Step [501/1000], Loss: 1.6299\n",
      "Epoch [50/150], Step [601/1000], Loss: 1.5573\n",
      "Epoch [50/150], Step [701/1000], Loss: 1.5567\n",
      "Epoch [50/150], Step [801/1000], Loss: 1.5301\n",
      "Epoch [50/150], Step [901/1000], Loss: 1.5356\n",
      "Epoch [50/150], Train Loss: 1.5686\n",
      "Epoch [50/150], Test Loss: 1.6759\n",
      "Test Accuracy: 78.52 %\n",
      "Epoch [51/150], Step [1/1000], Loss: 1.5900\n",
      "Epoch [51/150], Step [101/1000], Loss: 1.5085\n",
      "Epoch [51/150], Step [201/1000], Loss: 1.5288\n",
      "Epoch [51/150], Step [301/1000], Loss: 1.5700\n",
      "Epoch [51/150], Step [401/1000], Loss: 1.5464\n",
      "Epoch [51/150], Step [501/1000], Loss: 1.6316\n",
      "Epoch [51/150], Step [601/1000], Loss: 1.5314\n",
      "Epoch [51/150], Step [701/1000], Loss: 1.5855\n",
      "Epoch [51/150], Step [801/1000], Loss: 1.5520\n",
      "Epoch [51/150], Step [901/1000], Loss: 1.5729\n",
      "Epoch [51/150], Train Loss: 1.5657\n",
      "Epoch [51/150], Test Loss: 1.6832\n",
      "Test Accuracy: 77.83 %\n",
      "Epoch [52/150], Step [1/1000], Loss: 1.5154\n",
      "Epoch [52/150], Step [101/1000], Loss: 1.5628\n",
      "Epoch [52/150], Step [201/1000], Loss: 1.4936\n",
      "Epoch [52/150], Step [301/1000], Loss: 1.5462\n",
      "Epoch [52/150], Step [401/1000], Loss: 1.5653\n",
      "Epoch [52/150], Step [501/1000], Loss: 1.5036\n",
      "Epoch [52/150], Step [601/1000], Loss: 1.5745\n",
      "Epoch [52/150], Step [701/1000], Loss: 1.5683\n",
      "Epoch [52/150], Step [801/1000], Loss: 1.5930\n",
      "Epoch [52/150], Step [901/1000], Loss: 1.6488\n",
      "Epoch [52/150], Train Loss: 1.5628\n",
      "Epoch [52/150], Test Loss: 1.6765\n",
      "Test Accuracy: 78.46 %\n",
      "Epoch [53/150], Step [1/1000], Loss: 1.5857\n",
      "Epoch [53/150], Step [101/1000], Loss: 1.5324\n",
      "Epoch [53/150], Step [201/1000], Loss: 1.5412\n",
      "Epoch [53/150], Step [301/1000], Loss: 1.5518\n",
      "Epoch [53/150], Step [401/1000], Loss: 1.5622\n",
      "Epoch [53/150], Step [501/1000], Loss: 1.6084\n",
      "Epoch [53/150], Step [601/1000], Loss: 1.5397\n",
      "Epoch [53/150], Step [701/1000], Loss: 1.5858\n",
      "Epoch [53/150], Step [801/1000], Loss: 1.5518\n",
      "Epoch [53/150], Step [901/1000], Loss: 1.5551\n",
      "Epoch [53/150], Train Loss: 1.5627\n",
      "Epoch [53/150], Test Loss: 1.6829\n",
      "Test Accuracy: 77.9 %\n",
      "Epoch [54/150], Step [1/1000], Loss: 1.5418\n",
      "Epoch [54/150], Step [101/1000], Loss: 1.5633\n",
      "Epoch [54/150], Step [201/1000], Loss: 1.5808\n",
      "Epoch [54/150], Step [301/1000], Loss: 1.5672\n",
      "Epoch [54/150], Step [401/1000], Loss: 1.5116\n",
      "Epoch [54/150], Step [501/1000], Loss: 1.6356\n",
      "Epoch [54/150], Step [601/1000], Loss: 1.6409\n",
      "Epoch [54/150], Step [701/1000], Loss: 1.5244\n",
      "Epoch [54/150], Step [801/1000], Loss: 1.5848\n",
      "Epoch [54/150], Step [901/1000], Loss: 1.5359\n",
      "Epoch [54/150], Train Loss: 1.5612\n",
      "Epoch [54/150], Test Loss: 1.6807\n",
      "Test Accuracy: 77.96 %\n",
      "Epoch [55/150], Step [1/1000], Loss: 1.5484\n",
      "Epoch [55/150], Step [101/1000], Loss: 1.5360\n",
      "Epoch [55/150], Step [201/1000], Loss: 1.5900\n",
      "Epoch [55/150], Step [301/1000], Loss: 1.5378\n",
      "Epoch [55/150], Step [401/1000], Loss: 1.5165\n",
      "Epoch [55/150], Step [501/1000], Loss: 1.5661\n",
      "Epoch [55/150], Step [601/1000], Loss: 1.6274\n",
      "Epoch [55/150], Step [701/1000], Loss: 1.5692\n",
      "Epoch [55/150], Step [801/1000], Loss: 1.6113\n",
      "Epoch [55/150], Step [901/1000], Loss: 1.5225\n",
      "Epoch [55/150], Train Loss: 1.5569\n",
      "Epoch [55/150], Test Loss: 1.6730\n",
      "Test Accuracy: 78.78 %\n",
      "Epoch [56/150], Step [1/1000], Loss: 1.5098\n",
      "Epoch [56/150], Step [101/1000], Loss: 1.5474\n",
      "Epoch [56/150], Step [201/1000], Loss: 1.5904\n",
      "Epoch [56/150], Step [301/1000], Loss: 1.5199\n",
      "Epoch [56/150], Step [401/1000], Loss: 1.5250\n",
      "Epoch [56/150], Step [501/1000], Loss: 1.5526\n",
      "Epoch [56/150], Step [601/1000], Loss: 1.5277\n",
      "Epoch [56/150], Step [701/1000], Loss: 1.4934\n",
      "Epoch [56/150], Step [801/1000], Loss: 1.5946\n",
      "Epoch [56/150], Step [901/1000], Loss: 1.5041\n",
      "Epoch [56/150], Train Loss: 1.5581\n",
      "Epoch [56/150], Test Loss: 1.6740\n",
      "Test Accuracy: 78.59 %\n",
      "Epoch [57/150], Step [1/1000], Loss: 1.5873\n",
      "Epoch [57/150], Step [101/1000], Loss: 1.6157\n",
      "Epoch [57/150], Step [201/1000], Loss: 1.5608\n",
      "Epoch [57/150], Step [301/1000], Loss: 1.5374\n",
      "Epoch [57/150], Step [401/1000], Loss: 1.5216\n",
      "Epoch [57/150], Step [501/1000], Loss: 1.5358\n",
      "Epoch [57/150], Step [601/1000], Loss: 1.5723\n",
      "Epoch [57/150], Step [701/1000], Loss: 1.5885\n",
      "Epoch [57/150], Step [801/1000], Loss: 1.5369\n",
      "Epoch [57/150], Step [901/1000], Loss: 1.6053\n",
      "Epoch [57/150], Train Loss: 1.5536\n",
      "Epoch [57/150], Test Loss: 1.6748\n",
      "Test Accuracy: 78.75 %\n",
      "Epoch [58/150], Step [1/1000], Loss: 1.5538\n",
      "Epoch [58/150], Step [101/1000], Loss: 1.5589\n",
      "Epoch [58/150], Step [201/1000], Loss: 1.6077\n",
      "Epoch [58/150], Step [301/1000], Loss: 1.5692\n",
      "Epoch [58/150], Step [401/1000], Loss: 1.5249\n",
      "Epoch [58/150], Step [501/1000], Loss: 1.5087\n",
      "Epoch [58/150], Step [601/1000], Loss: 1.5987\n",
      "Epoch [58/150], Step [701/1000], Loss: 1.5593\n",
      "Epoch [58/150], Step [801/1000], Loss: 1.4880\n",
      "Epoch [58/150], Step [901/1000], Loss: 1.5114\n",
      "Epoch [58/150], Train Loss: 1.5508\n",
      "Epoch [58/150], Test Loss: 1.6730\n",
      "Test Accuracy: 78.9 %\n",
      "Epoch [59/150], Step [1/1000], Loss: 1.5388\n",
      "Epoch [59/150], Step [101/1000], Loss: 1.4919\n",
      "Epoch [59/150], Step [201/1000], Loss: 1.6089\n",
      "Epoch [59/150], Step [301/1000], Loss: 1.6326\n",
      "Epoch [59/150], Step [401/1000], Loss: 1.5201\n",
      "Epoch [59/150], Step [501/1000], Loss: 1.5632\n",
      "Epoch [59/150], Step [601/1000], Loss: 1.6397\n",
      "Epoch [59/150], Step [701/1000], Loss: 1.5620\n",
      "Epoch [59/150], Step [801/1000], Loss: 1.5140\n",
      "Epoch [59/150], Step [901/1000], Loss: 1.6018\n",
      "Epoch [59/150], Train Loss: 1.5502\n",
      "Epoch [59/150], Test Loss: 1.6722\n",
      "Test Accuracy: 79.08 %\n",
      "Epoch [60/150], Step [1/1000], Loss: 1.4984\n",
      "Epoch [60/150], Step [101/1000], Loss: 1.4912\n",
      "Epoch [60/150], Step [201/1000], Loss: 1.5023\n",
      "Epoch [60/150], Step [301/1000], Loss: 1.5773\n",
      "Epoch [60/150], Step [401/1000], Loss: 1.5062\n",
      "Epoch [60/150], Step [501/1000], Loss: 1.5238\n",
      "Epoch [60/150], Step [601/1000], Loss: 1.5226\n",
      "Epoch [60/150], Step [701/1000], Loss: 1.5509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/150], Step [801/1000], Loss: 1.5064\n",
      "Epoch [60/150], Step [901/1000], Loss: 1.4952\n",
      "Epoch [60/150], Train Loss: 1.5488\n",
      "Epoch [60/150], Test Loss: 1.6834\n",
      "Test Accuracy: 77.75 %\n",
      "Epoch [61/150], Step [1/1000], Loss: 1.5009\n",
      "Epoch [61/150], Step [101/1000], Loss: 1.5619\n",
      "Epoch [61/150], Step [201/1000], Loss: 1.5290\n",
      "Epoch [61/150], Step [301/1000], Loss: 1.5062\n",
      "Epoch [61/150], Step [401/1000], Loss: 1.5702\n",
      "Epoch [61/150], Step [501/1000], Loss: 1.4835\n",
      "Epoch [61/150], Step [601/1000], Loss: 1.5341\n",
      "Epoch [61/150], Step [701/1000], Loss: 1.5745\n",
      "Epoch [61/150], Step [801/1000], Loss: 1.5074\n",
      "Epoch [61/150], Step [901/1000], Loss: 1.5750\n",
      "Epoch [61/150], Train Loss: 1.5464\n",
      "Epoch [61/150], Test Loss: 1.6812\n",
      "Test Accuracy: 77.94 %\n",
      "Epoch [62/150], Step [1/1000], Loss: 1.5298\n",
      "Epoch [62/150], Step [101/1000], Loss: 1.5411\n",
      "Epoch [62/150], Step [201/1000], Loss: 1.5821\n",
      "Epoch [62/150], Step [301/1000], Loss: 1.6225\n",
      "Epoch [62/150], Step [401/1000], Loss: 1.5379\n",
      "Epoch [62/150], Step [501/1000], Loss: 1.5605\n",
      "Epoch [62/150], Step [601/1000], Loss: 1.5357\n",
      "Epoch [62/150], Step [701/1000], Loss: 1.5978\n",
      "Epoch [62/150], Step [801/1000], Loss: 1.5404\n",
      "Epoch [62/150], Step [901/1000], Loss: 1.4955\n",
      "Epoch [62/150], Train Loss: 1.5474\n",
      "Epoch [62/150], Test Loss: 1.6739\n",
      "Test Accuracy: 78.7 %\n",
      "Epoch [63/150], Step [1/1000], Loss: 1.5583\n",
      "Epoch [63/150], Step [101/1000], Loss: 1.5687\n",
      "Epoch [63/150], Step [201/1000], Loss: 1.5349\n",
      "Epoch [63/150], Step [301/1000], Loss: 1.4860\n",
      "Epoch [63/150], Step [401/1000], Loss: 1.5505\n",
      "Epoch [63/150], Step [501/1000], Loss: 1.5280\n",
      "Epoch [63/150], Step [601/1000], Loss: 1.5096\n",
      "Epoch [63/150], Step [701/1000], Loss: 1.5142\n",
      "Epoch [63/150], Step [801/1000], Loss: 1.5737\n",
      "Epoch [63/150], Step [901/1000], Loss: 1.5783\n",
      "Epoch [63/150], Train Loss: 1.5448\n",
      "Epoch [63/150], Test Loss: 1.6782\n",
      "Test Accuracy: 78.21 %\n",
      "Epoch [64/150], Step [1/1000], Loss: 1.5560\n",
      "Epoch [64/150], Step [101/1000], Loss: 1.5448\n",
      "Epoch [64/150], Step [201/1000], Loss: 1.4768\n",
      "Epoch [64/150], Step [301/1000], Loss: 1.4860\n",
      "Epoch [64/150], Step [401/1000], Loss: 1.6033\n",
      "Epoch [64/150], Step [501/1000], Loss: 1.5553\n",
      "Epoch [64/150], Step [601/1000], Loss: 1.5794\n",
      "Epoch [64/150], Step [701/1000], Loss: 1.5719\n",
      "Epoch [64/150], Step [801/1000], Loss: 1.5262\n",
      "Epoch [64/150], Step [901/1000], Loss: 1.6221\n",
      "Epoch [64/150], Train Loss: 1.5435\n",
      "Epoch [64/150], Test Loss: 1.6751\n",
      "Test Accuracy: 78.81 %\n",
      "Epoch [65/150], Step [1/1000], Loss: 1.5341\n",
      "Epoch [65/150], Step [101/1000], Loss: 1.5625\n",
      "Epoch [65/150], Step [201/1000], Loss: 1.4775\n",
      "Epoch [65/150], Step [301/1000], Loss: 1.6081\n",
      "Epoch [65/150], Step [401/1000], Loss: 1.6105\n",
      "Epoch [65/150], Step [501/1000], Loss: 1.5464\n",
      "Epoch [65/150], Step [601/1000], Loss: 1.5723\n",
      "Epoch [65/150], Step [701/1000], Loss: 1.5680\n",
      "Epoch [65/150], Step [801/1000], Loss: 1.5521\n",
      "Epoch [65/150], Step [901/1000], Loss: 1.5544\n",
      "Epoch [65/150], Train Loss: 1.5420\n",
      "Epoch [65/150], Test Loss: 1.6767\n",
      "Test Accuracy: 78.28 %\n",
      "Epoch [66/150], Step [1/1000], Loss: 1.5537\n",
      "Epoch [66/150], Step [101/1000], Loss: 1.4655\n",
      "Epoch [66/150], Step [201/1000], Loss: 1.5778\n",
      "Epoch [66/150], Step [301/1000], Loss: 1.6075\n",
      "Epoch [66/150], Step [401/1000], Loss: 1.4997\n",
      "Epoch [66/150], Step [501/1000], Loss: 1.5133\n",
      "Epoch [66/150], Step [601/1000], Loss: 1.5443\n",
      "Epoch [66/150], Step [701/1000], Loss: 1.4903\n",
      "Epoch [66/150], Step [801/1000], Loss: 1.6131\n",
      "Epoch [66/150], Step [901/1000], Loss: 1.5845\n",
      "Epoch [66/150], Train Loss: 1.5391\n",
      "Epoch [66/150], Test Loss: 1.6795\n",
      "Test Accuracy: 78.11 %\n",
      "Epoch [67/150], Step [1/1000], Loss: 1.5948\n",
      "Epoch [67/150], Step [101/1000], Loss: 1.5797\n",
      "Epoch [67/150], Step [201/1000], Loss: 1.5012\n",
      "Epoch [67/150], Step [301/1000], Loss: 1.4822\n",
      "Epoch [67/150], Step [401/1000], Loss: 1.6017\n",
      "Epoch [67/150], Step [501/1000], Loss: 1.5936\n",
      "Epoch [67/150], Step [601/1000], Loss: 1.4895\n",
      "Epoch [67/150], Step [701/1000], Loss: 1.5150\n",
      "Epoch [67/150], Step [801/1000], Loss: 1.5321\n",
      "Epoch [67/150], Step [901/1000], Loss: 1.5725\n",
      "Epoch [67/150], Train Loss: 1.5386\n",
      "Epoch [67/150], Test Loss: 1.6742\n",
      "Test Accuracy: 78.95 %\n",
      "Epoch [68/150], Step [1/1000], Loss: 1.5023\n",
      "Epoch [68/150], Step [101/1000], Loss: 1.5450\n",
      "Epoch [68/150], Step [201/1000], Loss: 1.5577\n",
      "Epoch [68/150], Step [301/1000], Loss: 1.5461\n",
      "Epoch [68/150], Step [401/1000], Loss: 1.5255\n",
      "Epoch [68/150], Step [501/1000], Loss: 1.5090\n",
      "Epoch [68/150], Step [601/1000], Loss: 1.5413\n",
      "Epoch [68/150], Step [701/1000], Loss: 1.5169\n",
      "Epoch [68/150], Step [801/1000], Loss: 1.5432\n",
      "Epoch [68/150], Step [901/1000], Loss: 1.5172\n",
      "Epoch [68/150], Train Loss: 1.5365\n",
      "Epoch [68/150], Test Loss: 1.6748\n",
      "Test Accuracy: 78.6 %\n",
      "Epoch [69/150], Step [1/1000], Loss: 1.5195\n",
      "Epoch [69/150], Step [101/1000], Loss: 1.4934\n",
      "Epoch [69/150], Step [201/1000], Loss: 1.4910\n",
      "Epoch [69/150], Step [301/1000], Loss: 1.4836\n",
      "Epoch [69/150], Step [401/1000], Loss: 1.5440\n",
      "Epoch [69/150], Step [501/1000], Loss: 1.5133\n",
      "Epoch [69/150], Step [601/1000], Loss: 1.5169\n",
      "Epoch [69/150], Step [701/1000], Loss: 1.5437\n",
      "Epoch [69/150], Step [801/1000], Loss: 1.4820\n",
      "Epoch [69/150], Step [901/1000], Loss: 1.5341\n",
      "Epoch [69/150], Train Loss: 1.5354\n",
      "Epoch [69/150], Test Loss: 1.6720\n",
      "Test Accuracy: 78.85 %\n",
      "Epoch [70/150], Step [1/1000], Loss: 1.5173\n",
      "Epoch [70/150], Step [101/1000], Loss: 1.5445\n",
      "Epoch [70/150], Step [201/1000], Loss: 1.4906\n",
      "Epoch [70/150], Step [301/1000], Loss: 1.5817\n",
      "Epoch [70/150], Step [401/1000], Loss: 1.5675\n",
      "Epoch [70/150], Step [501/1000], Loss: 1.5089\n",
      "Epoch [70/150], Step [601/1000], Loss: 1.5344\n",
      "Epoch [70/150], Step [701/1000], Loss: 1.5533\n",
      "Epoch [70/150], Step [801/1000], Loss: 1.5215\n",
      "Epoch [70/150], Step [901/1000], Loss: 1.5917\n",
      "Epoch [70/150], Train Loss: 1.5328\n",
      "Epoch [70/150], Test Loss: 1.6708\n",
      "Test Accuracy: 78.98 %\n",
      "Epoch [71/150], Step [1/1000], Loss: 1.5466\n",
      "Epoch [71/150], Step [101/1000], Loss: 1.5598\n",
      "Epoch [71/150], Step [201/1000], Loss: 1.5112\n",
      "Epoch [71/150], Step [301/1000], Loss: 1.4925\n",
      "Epoch [71/150], Step [401/1000], Loss: 1.5172\n",
      "Epoch [71/150], Step [501/1000], Loss: 1.5565\n",
      "Epoch [71/150], Step [601/1000], Loss: 1.5792\n",
      "Epoch [71/150], Step [701/1000], Loss: 1.5136\n",
      "Epoch [71/150], Step [801/1000], Loss: 1.5547\n",
      "Epoch [71/150], Step [901/1000], Loss: 1.5399\n",
      "Epoch [71/150], Train Loss: 1.5316\n",
      "Epoch [71/150], Test Loss: 1.6725\n",
      "Test Accuracy: 78.98 %\n",
      "Epoch [72/150], Step [1/1000], Loss: 1.5215\n",
      "Epoch [72/150], Step [101/1000], Loss: 1.5879\n",
      "Epoch [72/150], Step [201/1000], Loss: 1.5074\n",
      "Epoch [72/150], Step [301/1000], Loss: 1.5460\n",
      "Epoch [72/150], Step [401/1000], Loss: 1.5855\n",
      "Epoch [72/150], Step [501/1000], Loss: 1.5131\n",
      "Epoch [72/150], Step [601/1000], Loss: 1.5683\n",
      "Epoch [72/150], Step [701/1000], Loss: 1.6036\n",
      "Epoch [72/150], Step [801/1000], Loss: 1.5052\n",
      "Epoch [72/150], Step [901/1000], Loss: 1.5752\n",
      "Epoch [72/150], Train Loss: 1.5326\n",
      "Epoch [72/150], Test Loss: 1.6751\n",
      "Test Accuracy: 78.58 %\n",
      "Epoch [73/150], Step [1/1000], Loss: 1.4917\n",
      "Epoch [73/150], Step [101/1000], Loss: 1.5021\n",
      "Epoch [73/150], Step [201/1000], Loss: 1.4792\n",
      "Epoch [73/150], Step [301/1000], Loss: 1.5448\n",
      "Epoch [73/150], Step [401/1000], Loss: 1.5373\n",
      "Epoch [73/150], Step [501/1000], Loss: 1.5433\n",
      "Epoch [73/150], Step [601/1000], Loss: 1.5044\n",
      "Epoch [73/150], Step [701/1000], Loss: 1.5236\n",
      "Epoch [73/150], Step [801/1000], Loss: 1.5749\n",
      "Epoch [73/150], Step [901/1000], Loss: 1.5717\n",
      "Epoch [73/150], Train Loss: 1.5309\n",
      "Epoch [73/150], Test Loss: 1.6733\n",
      "Test Accuracy: 78.79 %\n",
      "Epoch [74/150], Step [1/1000], Loss: 1.4671\n",
      "Epoch [74/150], Step [101/1000], Loss: 1.5536\n",
      "Epoch [74/150], Step [201/1000], Loss: 1.5228\n",
      "Epoch [74/150], Step [301/1000], Loss: 1.5120\n",
      "Epoch [74/150], Step [401/1000], Loss: 1.5787\n",
      "Epoch [74/150], Step [501/1000], Loss: 1.5311\n",
      "Epoch [74/150], Step [601/1000], Loss: 1.5649\n",
      "Epoch [74/150], Step [701/1000], Loss: 1.5171\n",
      "Epoch [74/150], Step [801/1000], Loss: 1.5007\n",
      "Epoch [74/150], Step [901/1000], Loss: 1.5462\n",
      "Epoch [74/150], Train Loss: 1.5306\n",
      "Epoch [74/150], Test Loss: 1.6744\n",
      "Test Accuracy: 78.81 %\n",
      "Epoch [75/150], Step [1/1000], Loss: 1.5451\n",
      "Epoch [75/150], Step [101/1000], Loss: 1.4954\n",
      "Epoch [75/150], Step [201/1000], Loss: 1.5403\n",
      "Epoch [75/150], Step [301/1000], Loss: 1.5291\n",
      "Epoch [75/150], Step [401/1000], Loss: 1.5008\n",
      "Epoch [75/150], Step [501/1000], Loss: 1.4946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/150], Step [601/1000], Loss: 1.5317\n",
      "Epoch [75/150], Step [701/1000], Loss: 1.4789\n",
      "Epoch [75/150], Step [801/1000], Loss: 1.5566\n",
      "Epoch [75/150], Step [901/1000], Loss: 1.4920\n",
      "Epoch [75/150], Train Loss: 1.5284\n",
      "Epoch [75/150], Test Loss: 1.6708\n",
      "Test Accuracy: 79.22 %\n",
      "Epoch [76/150], Step [1/1000], Loss: 1.4749\n",
      "Epoch [76/150], Step [101/1000], Loss: 1.5252\n",
      "Epoch [76/150], Step [201/1000], Loss: 1.4919\n",
      "Epoch [76/150], Step [301/1000], Loss: 1.5178\n",
      "Epoch [76/150], Step [401/1000], Loss: 1.5320\n",
      "Epoch [76/150], Step [501/1000], Loss: 1.5958\n",
      "Epoch [76/150], Step [601/1000], Loss: 1.5394\n",
      "Epoch [76/150], Step [701/1000], Loss: 1.4880\n",
      "Epoch [76/150], Step [801/1000], Loss: 1.5699\n",
      "Epoch [76/150], Step [901/1000], Loss: 1.4956\n",
      "Epoch [76/150], Train Loss: 1.5263\n",
      "Epoch [76/150], Test Loss: 1.6756\n",
      "Test Accuracy: 78.38 %\n",
      "Epoch [77/150], Step [1/1000], Loss: 1.4626\n",
      "Epoch [77/150], Step [101/1000], Loss: 1.5384\n",
      "Epoch [77/150], Step [201/1000], Loss: 1.5244\n",
      "Epoch [77/150], Step [301/1000], Loss: 1.4696\n",
      "Epoch [77/150], Step [401/1000], Loss: 1.5205\n",
      "Epoch [77/150], Step [501/1000], Loss: 1.4836\n",
      "Epoch [77/150], Step [601/1000], Loss: 1.5039\n",
      "Epoch [77/150], Step [701/1000], Loss: 1.5013\n",
      "Epoch [77/150], Step [801/1000], Loss: 1.4840\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 150\n",
    "total_steps = len(trainloader)\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "# loop through epochs\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_running_loss = 0  # track train running loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.train() # set to train mode\n",
    "    \n",
    "    # load batch images/labels\n",
    "    for step, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        # put data onto available device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = model(images)  # forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # retrieve top preds\n",
    "        \n",
    "        total += labels.size(0)  # add batch size\n",
    "        correct += (predicted == labels).sum().item()  # calc num correct\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # calc loss\n",
    "        train_running_loss += loss.item()  # acc running loss\n",
    "        \n",
    "        loss.backward()   # backprop\n",
    "        optimizer.step()  # forward\n",
    "\n",
    "        if step % 100 == 0:  # print progress by iteration\n",
    "        \n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "            .format(epoch+1, epochs, step+1, total_steps, loss.item()))\n",
    "    \n",
    "    # div by num batches to get average\n",
    "    epoch_train_loss = train_running_loss / len(trainloader)\n",
    "                   \n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}'.format(epoch+1, epochs, epoch_train_loss))\n",
    "        \n",
    "    # append the loss/acc after all the steps \n",
    "    training_losses.append(epoch_train_loss)\n",
    "    train_acc.append(correct / total)\n",
    "        \n",
    "    \n",
    "    # ------------------------------ #\n",
    "    \n",
    "\n",
    "    # evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_running_loss = 0  # track test running loss\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in testloader:\n",
    "                   \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "                   \n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "                   \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "                 \n",
    "    # div by num batches\n",
    "    epoch_test_loss = test_running_loss / len(testloader)\n",
    "                   \n",
    "    print('Epoch [{}/{}], Test Loss: {:.4f}'.format(epoch+1, epochs, epoch_test_loss))\n",
    "        \n",
    "    # append the loss & acc after all the steps \n",
    "    test_losses.append(epoch_test_loss)\n",
    "    test_acc.append(correct / total)\n",
    "            \n",
    "    print('Test Accuracy: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(train_losses, test_losses, train_acc, test_acc):\n",
    "    # plot graph\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(test_losses, label=\"Test loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Loss vs Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "    plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Accuracy vs Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "    plt.savefig('part_a_cifar_loss_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(training_losses, test_losses, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve conv1 layer, convert to numpy array, put on cpu first\n",
    "weights = model.conv1.weight.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_stretch(filter):\n",
    "    \n",
    "    filter = filter - torch.min(filter)\n",
    "    \n",
    "    return filter / torch.max(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot filters\n",
    "def plot_kernels(tensor, num_rows=8, num_cols=8):\n",
    "\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)  \n",
    "        \n",
    "        # need to reshape it to move channels to end\n",
    "        reshaped = tensor[i].view(11,11,3)\n",
    "        norm_tensor = norm_stretch(reshaped)\n",
    "        \n",
    "#         print(norm_tensor.shape)\n",
    "        \n",
    "#         ax1.imshow(tensor[i])\n",
    "        ax1.imshow(norm_tensor)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "    \n",
    "#     plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.savefig('filters_part3b.jpg')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
